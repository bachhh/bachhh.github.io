<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title> Distributed Hash Table </title>
  <meta name="viewport" content="width=device-width">

  <!-- syntax highlighting CSS -->
  <link rel="stylesheet" href="/css/syntax.css">
  <link rel="stylesheet" href="/css/codeblock.scss">


  <!-- CSS -->
  <link rel="stylesheet" href="/css/reset.css">
  <link rel="stylesheet" href="/css/main.css">
  <link href="http://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

   <!--- MathJax -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <!-- Fonts -->
  <link href='http://fonts.googleapis.com/css?family=Bitter:400,700,400italic|Open+Sans:400italic,600italic,400,600' rel='stylesheet' type='text/css'>
</head>
<body>
  <header style="background-image: url(/images/akira.jpg);">
  <div class="container post-container">
    <a href="/" class="home_button"></a>
    <div class="inner-container">
      <h1>Distributed Hash Table</h1>
      <ul class="meta">
        <li>
          <span>
            Published
          </span>
          08 Mar 2022
        </li>
        <li>
          <span>
            Category
          </span>
          blog
        </li>
        <li>
          <span>
            Tags
          </span>
          
        </li>

      </ul>
    </div>

  </div>
</header>

<article>
  <div class="container">
    <p>This is my note but I rewrote it to a publishable form :-?.</p>

<h1 id="distributed-hash-table">Distributed Hash Table</h1>

<p>The distributed Hash Table problem is pretty much what it sounds like: a hash table distributed throughout multiple machines in a network. The Hash Table answers this query for any given object:</p>

<p>Given an object and n buckets/nodes/servers, on what bucket do I put this object into ?</p>

<p>In a typical HashMap library or HashIndex in databases, the query is implemented with a very simple hash function and modulo ops: <code>bucket = hash(o) % n</code>, although caveats such as <a href="https://en.wikipedia.org/wiki/Linear_probing">linear probing</a>, <a href="https://en.wikipedia.org/wiki/Hash_table#Separate_chaining">chaining</a>, or <a href="https://en.wikipedia.org/wiki/Cuckoo_hashing">Cuckoo</a> applies when more than 1 object produce the same bucket.</p>

<p>This approach works well in a non-orchestrator context, no single entity is
responsible for choosing which server store an object. Assuming client A put
write the Object in server 1, and later client B read it. B knows immediately and 
without asking A, that ojbect O belongs to server 1, B connect and ask for it’s
value directly.</p>

<p>But the naive approach is bad, we can look at the behaviour of a cluster of
node to see how bad it is.</p>

<!-- < example of mass rehashing during migration > -->
<p>Assume we have 4 nodes: A B C and D, node D goes offline ( maybe bad network )
for 5 minutes, then it’s backup again.</p>

<table>
  <thead>
    <tr>
      <th>key</th>
      <th>Dest (normal) -</th>
      <th>New Dest ( downtime )</th>
      <th>Moved</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>key0</td>
      <td>node1:A</td>
      <td>node2:A</td>
      <td>moved:False</td>
    </tr>
    <tr>
      <td>key1</td>
      <td>node1:B</td>
      <td>node2:B</td>
      <td>moved:False</td>
    </tr>
    <tr>
      <td>key2</td>
      <td>node1:C</td>
      <td>node2:C</td>
      <td>moved:False</td>
    </tr>
    <tr>
      <td>key3</td>
      <td>node1:D</td>
      <td>node2:A</td>
      <td>moved:True</td>
    </tr>
    <tr>
      <td>key4</td>
      <td>node1:A</td>
      <td>node2:B</td>
      <td>moved:True</td>
    </tr>
    <tr>
      <td>key5</td>
      <td>node1:B</td>
      <td>node2:C</td>
      <td>moved:True</td>
    </tr>
    <tr>
      <td>key6</td>
      <td>node1:C</td>
      <td>node2:A</td>
      <td>moved:True</td>
    </tr>
    <tr>
      <td>key7</td>
      <td>node1:D</td>
      <td>node2:B</td>
      <td>moved:True</td>
    </tr>
    <tr>
      <td>key8</td>
      <td>node1:A</td>
      <td>node2:C</td>
      <td>moved:True</td>
    </tr>
    <tr>
      <td>key9</td>
      <td>node1:B</td>
      <td>node2:A</td>
      <td>moved:True</td>
    </tr>
    <tr>
      <td>key10</td>
      <td>node1:C</td>
      <td>node2:B</td>
      <td>moved:True</td>
    </tr>
    <tr>
      <td>key11</td>
      <td>node1:D</td>
      <td>node2:C</td>
      <td>moved:True</td>
    </tr>
    <tr>
      <td>key12</td>
      <td>node1:A</td>
      <td>node2:A</td>
      <td>moved:False</td>
    </tr>
    <tr>
      <td>key13</td>
      <td>node1:B</td>
      <td>node2:B</td>
      <td>moved:False</td>
    </tr>
    <tr>
      <td>key14</td>
      <td>node1:C</td>
      <td>node2:C</td>
      <td>moved:False</td>
    </tr>
  </tbody>
</table>

<p>In total, 9 keys were moved during 5 minutes, that’s over half total.</p>

<p>The problem is amplified when server come on and off-line few times every
minutes ( or seconds, depending on how tolerant you’re to network lag). So
every 5 minutes, more than half of our keys are shuffled around the cluster. I
can imagine a case where the down node is back online before we are even done
with the object transfer process.</p>

<p>So people came up with a concept of <strong>minimal disruption</strong> property for these
distributed datastructures: Given a cluster of M nodes and K object, each node
storing M/K keys-value object. In case 1 node goes offline, if only M/K keys are
expected to be moved from it’s original position, then the cluster achieved</p>

<p><strong>minimal disruption</strong>.
It’s fancy way to say: don’t move around more keys than what’s a single down
node have.</p>

<pre><code>A distributed system is one in which the failure of a computer you did not even
know existed can render your own computer unusable.
- funni Leslie Lamport quote
</code></pre>

<p>In an ideal Distributed Hash Table, when there’s a change to our cluster’s
members, our hash entries should remain ( some what ) unchanged. 
A server leaving the system should not cause redistribution of other server’s
data, only it’s own. A new server join should only “steal” as many data from others as
it’s full capacity. The problem of DHT calls for a different family of hashing scheme,
the Stable Hashing scheme.</p>

<h2 id="redis">Redis</h2>

<p>Before consistent hash,
<a href="https://redis.io/docs/manual/scaling/#redis-cluster-data-sharding">Redis</a>
employ a pretty fun method of for mitigate the problem of key-rehash during migration:</p>

<p>Intead of hashing individual key, Redis packs them into bin, which they termed <strong>Hash Slot</strong>. There are 16384 hash slots in total. For a given key, <code>hash slot</code> is chosen using <code>CRC16(key) % 16384</code>.</p>

<p>The 16384 slots is then distributed to real physicals nodes. So in a way, we
can see <strong>Hash Slot</strong> as some kind of a <strong>logical nodes</strong>, not too dissimilar
to other Consistent Hash methods below.</p>

<pre><code>+---+
|   | --&gt; [ slot 1]---&gt; Node 1
|   | --&gt; [ slot 2]/
|key| --&gt; [ slot 3]\ 
|   | --&gt; [ slot 4]---&gt; Node 2
|   | --&gt; [ slot 5]/
+---+
</code></pre>

<p>The number 16384 is chosen based on engineering <a href="https://github.com/redis/redis/issues/2576">design</a>: 
A node specify it’s <strong>Hash Slot</strong> configuration as a <strong>bitmap</strong>: If node A is the owner of 
hash slot 0, 3 and 7, it’s bitmap would be <code>10001001</code>  ( big-endian ).</p>
<ul>
  <li>With 65k hash slots, the size of this <em>info bitmap</em> would be 8KB.</li>
  <li>With 16384 hash slots, the size of this <em>info bitmap</em> would only be 2KB.</li>
</ul>

<p>On why shouldn’t Redis goes wiht smaller slots number: the higher number of
slots, the more masters you’re allowed to have. Theoretically you could have
16384 masters with 1 slot each, or 1000 masters with 16 slots each.</p>

<p>Side notes:</p>
<blockquote>
  <p>Note that in small clusters the bitmap would be hard to compress because when
N is small the bitmap would have slots/N bits set that is a large percentage of
bits set.</p>
</blockquote>

<p>I don’t quite follow this reasoning since bitmap compression like LRE achieve
good ratio based on <em>repeating bit pattern</em> rather than the pure number of bit set.
i.e.: <code>11111111100000000</code> gets more compression than <code>010101010101010</code>.
So if the mechanism for a node claiming Hash Slot is by range, LRE should be good ?</p>

<h2 id="highest-random-weight">Highest Random Weight</h2>

<p><a href="http://www.eecs.umich.edu/techreports/cse/96/CSE-TR-316-96.pdf">Thaler &amp; Ravishankar Section 4.1</a>
 in 1996 described the Highest Random Weight Hashing scheme. Although writtern as a
request-server mapping, the idea can be applied for object storage as well.</p>

<p>The idea is that, instead of producing a <strong>Hash</strong> value for the object then
choose the node based on that Hash value, the hash function now output a
<strong>Weight Score</strong> for each pair of <strong>(Object, Node)</strong>. Destination is chosen as
the value with highest weight.</p>

\[F() Object \rightarrow Node[i] = 
   \\
   Weight(Object, Node[i]) &gt; Weight(Object, Node[j]) 
   for all i, j in the set of Node [0, n]\]

<table>
  <thead>
    <tr>
      <th>Object/Key</th>
      <th>Node</th>
      <th>Score(Node, Object)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>K</td>
      <td>A</td>
      <td>12</td>
    </tr>
    <tr>
      <td>K</td>
      <td>B</td>
      <td>2</td>
    </tr>
    <tr>
      <td>K</td>
      <td>C</td>
      <td>8</td>
    </tr>
    <tr>
      <td>K</td>
      <td>D</td>
      <td>32       Winner</td>
    </tr>
  </tbody>
</table>

<p>HRW helps us achieve the <strong>minimal disruption</strong> property during migration:</p>

<p>When node i exit, all objected stored on that node will be evenly
redistributed. Since Weight(Object, Node[i]) is no longer the maximum value
in the set, we pick the second highest weight from all the remaining nodes.</p>

<p>When node k enter, all objects are re-hashed with new node’s value 
Weight(Object, Node[i]) ( assuming the cost of computing hash key is minimal
compare to the cost of moving data ), only the object which satisfies the
condition:</p>

\[Weight(Object, Node[i]) &gt; Weight(Object, Node[j]) 
    \\ 
    \text{for all i, j in [0, n]}\]

<p>Get redistributed, all other object stay intact.</p>

<p>On side note, another useful property of HRW is that it can solves the <strong>k-agreement
problem</strong>: Given a set of n possible values, all client must come to the
conclusion of choosing a set k out of n value. The k-agreement problem is the
generalized version of the Consensus Problem, where in the consensus case k = 1.
I think this one is very applicable for replica setup: every Key inserted into our
table will have a leader node, with k-1 other replica node. We also have a
deterministic process of promoting replica node as the next leader: In case the
leader goes down, replica with the 2nd highest weight is used as a query point
for our key.</p>

<h2 id="consistent-hashing">Consistent Hashing</h2>

<p><a href="https://www.cs.princeton.edu/courses/archive/fall09/cos518/papers/chash.pdf">Krager et al</a>
described a another algorithm to solve the problem of distributed hashing.</p>

<p>The basic operation is this: We hash a key to produce a value on some unit
interval range ( the paper use \([0.0, 1.0]\), but 64-bit hash function could map into
\([0, 2^{63}-1]\) interval )</p>

<p>The interval is circular ( using modulo arithmatic ) and can be visualized as a
ring. Our <strong>nodes</strong> will be placed at various points on the ring according to their hash value</p>

<p><img src="/images/toptal-consistent-hash-1.png" alt="Consistent Hash Ring, courtesy of Toptal" width="750" /></p>

<p><em>Borrowed image from Toptal</em></p>

<p>To find the destination node <code>node = Dest(key)</code> for a given key.</p>
<ul>
  <li>First we obtain the hash value of our key <code>h = Hash(key)</code></li>
  <li>Lookup the next node clockwise, with node hash value greater than key hash value.
    <ul>
      <li>In implementation, this is a binary search on the array of node’s hash value with \(O(logn)\)</li>
    </ul>
  </li>
</ul>

<p><strong>Weighted Balance</strong>: In practice, the distribution of the Node Hash
<code>Hash()</code> is quite uneven at small N values. To mitigate this the hashing scheme
split a physical node into multiple <strong>virtual nodes</strong>, each with it’s own <strong>Virtual ID</strong> and hash.</p>

<p><img src="/images/toptal-consistent-hash-2.png" alt="Consistent Hash Ring, courtesy of Toptal" width="750" /></p>

<p><em>Borrowed image from Toptal</em></p>

<p>Now the distribution of server get increasingly more even as N values is multiplied. 
To account for the difference in workload of nodes (i.e. some nodes can store more data than others),
the split ratio of physical - virtual node can be adjusted accordingly. Node with higher score
get splited into more virtual nodes ( and get higher % of being selected ).</p>

<p>The paper then goes on to show the following properties are achieved by the algorithm.</p>

<ul>
  <li><strong>Balance</strong> : Given a set of I items and B nodes, each of them will have approximately <code>1/|I|</code> items hashed to.</li>
  <li><strong>Monotonic</strong> : Given a set of V1 nodes, if some new x nodes are added,
keys will only be shuffling from old nodes to new nodes, not between old nodes.
    <ul>
      <li>Equivalent to <strong>minimal disruption</strong></li>
    </ul>
  </li>
</ul>

<p>And also having additional properties:</p>
<ul>
  <li><strong>Small Load</strong>: Formally, the <em>load</em> of of a hash function is the maximum load of any given bucket in our cluster.
    <ul>
      <li>We can understand it as for each machine, the number of object it stores also should be small.</li>
      <li>A good hashing scheme will do their best to minimize <strong>load factor</strong> in the clusters.</li>
    </ul>
  </li>
  <li><strong>Small Spread</strong>: An object can be hashed to multiple possible nodes,
  depending on the <em>view</em> of the client. Small spread kept this possibilities
  small.
    <ul>
      <li>The concept of <strong>view</strong> is the number of available nodes from a client’s
  perspective, sometimes due to network failure a client may not be able to
  see every nodes in the cluster.</li>
      <li>Small spread can be translate as: it does not matter if the clients can
  only see half of the cluster. All clients will agree on some
  small set of destination nodes, for any given key.</li>
      <li>If spread is large, a key may have to be duplicated on multiple nodes to
  ensure that every clients ( and their views ) can lookup the key without
  cache miss.</li>
    </ul>
  </li>
</ul>

<h2 id="comments">Comments</h2>

<p><strong>Recomputation</strong>: In both HRW and Consistent Hash, the entire set of key will have to
Notice that in both HRW and Consistent Hash, the entire set of key will have to
be re-computed everytime the topology of our network changes. When a new node
enter, each key will re-compute Score(Object, Node[i]) to determine
if it should be moved or not. And in Consistent Hash, the interval <code>| Hash(Object) - Hash(Node[i])|</code>
will be recalculated to determine the closest node. The assumption here is that re-computing object key
cost is minimal, compare to the cost of moving key-value data during migration.</p>

<p>In hindsight, Rendezvous Hash is a generalization of Consistent Hashing: the Score function
Score(Object, Node[i]) can defined as such <code>| Hash(Object) - Hash(Node[i])|</code>.
Choosing the k-agreemment node in Consistent Hash is taking the k-minimum of all interval value.</p>

<p>The Computational cost is O(N) with N as the number of servers, under the usual
applied usecase of DHT ( as a distributed k-v storage ), O(N) should not be a
concerns. When balancing storage between 10-20 servers, O(N) is good enough.</p>

<p>One small difference though, is that HRW can somewhat be performed as a
<strong>stateless</strong> protocol, while Consistent Hash requires a shared state between
requester. The Consistent Hash requires a table of Node-Hash(Node) and updated
accordingly each time a migration happens. HRW avoid keeping this shared state,
but pay in other penalty: computing all pair hash Hash(object, Server[i]) each
time they query, but again, it’s insignificant on few dozens of nodes.</p>

<h2 id="weighted-variants">Weighted Variants</h2>

<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.414.9353&amp;rep=rep1&amp;type=pdf">Schindelhauer &amp; Schomaker</a> 
specifiy special methods for adapting Rendezvous Hash in weighted node context.</p>

<p>For Consistent Hash, the simple approach of multiplying physical to virtual node by some weight
factor. Then your hash function will get <code>xWeight</code> more probability of getting
a higher score than lower weight nodes.</p>

<p>This, however, get the downside of increasing hash computation linearly, especially
as the most common multiple of all weights approach prime number.</p>

<h2 id="carp">CARP</h2>

<p>Unlike Consistent Hash, for weighted-WRH (We can call it Weighted Rendezvous
Hash, although the acronym being the same is quite confusing), you don’t need
to commit yourself to doing multiplication arithematic. In CARP the score are
multiplied by a weight factor for a final score. The weight factor of scores
are relatively scaled so that their total sum = 1.0.</p>

<p>However note that this messes with the <strong>minimal disruption</strong> and <strong>monotonic</strong>
requirement: when node enter / exit under a migration cycle. Since weight
factor are relatively scaled, all score has to be recomputed with new weight
factor, and this causes more than N/M entries to be moving between nodes.</p>

<h2 id="weighted-stable--balanced-rendezvous-hash">Weighted Stable &amp; Balanced Rendezvous Hash</h2>

<p><a href="https://www.snia.org/sites/default/files/SDC15_presentations/dist_sys/Jason_Resch_New_Consistent_Hashings_Rev.pdf">Jason Resch</a>
points out another interesting method of weighted rendezvous hashing that compensated for 
the above drawback.</p>

<p>Instead of multiply by a relative scale factor, the score function is calculated by</p>

\[\text{let  y = (item|node[i])}
\\
Score(y) = \frac{w_i}{-\ln(hash(y))}
\\
\text{where } w_i \text{ is the weight of node[i]}
\\
\text{and } hash(i) = \text{the ideal hash function} (item|node[i]) \rightarrow [0, 1]\]

<p>Chosen node is node with highest score.</p>

<p>The neat thing about this function is that it provide the <strong>balance</strong> property, 
e.g.: the probability of data going to node[i] is proportion to the node’s weight</p>

\[Prob(\text{item goes into node i }) = \frac{w_i}{\sum_{k}^{n}w_k}\]

<p>While simutanuously ensure scores of unchanged node stay the same, meaning
nodes without weight change will not be unecessary exchanging data. This hashing scheme 
provice both <strong>stability</strong> ( no unecessary data shuffles ) and <strong>balance</strong>
(data distributed evenly). The proof of this property provided in Jason Resch’s slide.</p>

<h2 id="trivia">Trivia</h2>

<ul>
  <li><strong><a href="https://stackoverflow.com/a/67578565">DOS mitigation</a></strong> : Since HRW /
Consistent Hash map same request to the same server. In case of DOS attack,
round-robin spreading the load on all servers can brought down the whole
cluster. But for consistent hash, where the same request map to same server,
only a section of the server is down.</li>
</ul>

<p>This seems true at first, but I highly doubt DOS attacker didn’t already spoof
their packets to circumvent firewalls. When packet are spoofed, distribution of
packet on clusters become uniform, and net effect is no different from
round-robin.</p>

<ul>
  <li><strong>Migration</strong> A cool tricks you can do with Weighted Consistent/Rendezvous
Hash is to enable continuous migration during operation. For example, to retire
an old node from the cluster, you can gradually lower the weight factor of said
node until it reaches zero. By contrast, if unplugging the node outright, a
huge spike of data transfer in cluster network is ensued.</li>
</ul>

<p>This sounds like a good practice and I feel like it’s very applicable in
software operations.</p>

<h1 id="footnotes">Footnotes</h1>

<p>All of these is under the assumption that we already know the set of all
servers. When we don’t, the problem is caled <a href="">service discovery</a></p>

  </div>
</article>

</body>
</html>
